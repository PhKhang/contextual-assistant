{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "395c1a44",
   "metadata": {},
   "source": [
    "# Scrape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a52cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q requests markdownify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed92d1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from markdownify import markdownify as md\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "BASE_URL = \"https://support.optisigns.com/api/v2/help_center/articles.json\"\n",
    "OUTPUT_DIR = \"articles_md\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def slugify(url):\n",
    "    parsed = urlparse(url)\n",
    "    slug = parsed.path.rstrip(\"/\").split(\"/\")[-1]\n",
    "    return slug.replace(\"-\", \"_\")\n",
    "\n",
    "\n",
    "def fetch_articles():\n",
    "    articles = []\n",
    "    url = BASE_URL\n",
    "    while url:\n",
    "        resp = requests.get(url)\n",
    "        data = resp.json()\n",
    "        articles.extend(data.get(\"articles\", []))\n",
    "        url = data.get(\"next_page\")  # pagination\n",
    "    return articles\n",
    "\n",
    "\n",
    "def save_article_as_md(article):\n",
    "    html_content = article.get(\"body\", \"\")\n",
    "    markdown_content = md(html_content)\n",
    "\n",
    "    slug = slugify(article.get(\"html_url\", f\"article_{article['id']}\"))\n",
    "    filename = os.path.join(OUTPUT_DIR, f\"{slug}.md\")\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"# {article.get('title')}\\n\\n\")\n",
    "        f.write(markdown_content)\n",
    "\n",
    "\n",
    "def scrape_all():\n",
    "    print(\"Fetching articles...\")\n",
    "    articles = fetch_articles()\n",
    "    print(f\"Total articles fetched: {len(articles)}\")\n",
    "\n",
    "    for article in articles:\n",
    "        save_article_as_md(article)\n",
    "\n",
    "    print(f\"Saved all Markdown files in '{OUTPUT_DIR}' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecca58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65765c7b",
   "metadata": {},
   "source": [
    "## Chunking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035e4595",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c28934",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90a67a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def get_files_in_directory(directory):\n",
    "    files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".md\") or filename.endswith(\".json\"):\n",
    "            files.append(os.path.join(directory, filename))\n",
    "    return files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2feb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "import rich\n",
    "\n",
    "md_files = get_files_in_directory(OUTPUT_DIR)\n",
    "\n",
    "JSONL_DIR = \"jsonl_files\"\n",
    "os.makedirs(JSONL_DIR, exist_ok=True)\n",
    "\n",
    "for file in md_files:\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "        metadata = {\n",
    "            \"file_name\": os.path.basename(file),\n",
    "            \"file_path\": file,\n",
    "            \"chunk_index\": 0,\n",
    "        }\n",
    "        # print(metadata)\n",
    "\n",
    "        splitter = MarkdownTextSplitter(chunk_size=800, chunk_overlap=200)\n",
    "        chunks = splitter.split_text(content)\n",
    "        outfile = os.path.join(\n",
    "            JSONL_DIR, os.path.splitext(file)[0].split(os.sep)[-1] + \".json\"\n",
    "        )\n",
    "        # print(\"outfile:\", outfile, type(outfile))\n",
    "\n",
    "        with open(outfile, \"w\", encoding=\"utf-8\") as out:\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunk_metadata = metadata.copy()\n",
    "                chunk_metadata[\"chunk_index\"] = i\n",
    "                chunk_metadata[\"content\"] = chunk\n",
    "                # rich.print(chunk_metadata)\n",
    "                out.write(json.dumps(chunk_metadata, ensure_ascii=False) + \"\\n\")\n",
    "                # break\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7452fa8",
   "metadata": {},
   "source": [
    "# OpenAI Vector store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7babe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q openai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9411696",
   "metadata": {},
   "source": [
    "## Create OpenAI Client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86899715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305fe0c0",
   "metadata": {},
   "source": [
    "## Create Vector Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4732e816",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = client.vector_stores.create(name=\"Support FAQ\")\n",
    "print(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45599e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_id = \"vs_687a68688564819184ac96b514b8f083\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b3a1dc",
   "metadata": {},
   "source": [
    "## Check vector store storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fbf14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_stores = client.vector_stores.list()\n",
    "print(vector_stores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b778a72c",
   "metadata": {},
   "source": [
    "## Upload files to vector store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61e75d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_files_in_directory(OUTPUT_DIR)\n",
    "file_streams = [open(file, \"rb\") for file in files[:2]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367237b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50ec43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_batch = client.vector_stores.file_batches.upload_and_poll(\n",
    "    vector_store_id=vector_store_id, files=file_streams[:1]\n",
    ")\n",
    "print(file_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8882289",
   "metadata": {},
   "source": [
    "## Individual file upload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a18ea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.files.create(\n",
    "    file=open(\n",
    "        \"a.jsonl\",\n",
    "        \"rb\",\n",
    "    ),\n",
    "    purpose=\"assistants\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc6a0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0bbd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.vector_stores.file_batches.create(\n",
    "    vector_store_id=vector_store_id, file_ids=[\"file-2casDiXq1CLeUXPnX9Ad3U\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ab62ca",
   "metadata": {},
   "source": [
    "## Clear all files in vector store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a92f4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rich\n",
    "\n",
    "rich.print(client.files.list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d30327",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ids = [file.id for file in client.files.list()]\n",
    "print(len(file_ids), \"files uploaded to the vector store.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f784c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_id in file_ids:\n",
    "    client.files.delete(file_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb516123",
   "metadata": {},
   "source": [
    "## Update files in vector store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa16515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.vector_stores.files.retrieve(extra_query={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535fb490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rich\n",
    "\n",
    "vector_store_files = client.vector_stores.files.list(vector_store_id=vector_store_id)\n",
    "print(len(vector_store_files.data))\n",
    "rich.print(vector_store_files)\n",
    "\n",
    "id_to_delete = vector_store_files.data[0].id\n",
    "print(f\"Deleting file with ID: {id_to_delete}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd78f313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The client.vector_stores.files.delete does not actuall the file but removes it from the vector store\n",
    "# deleted_vector_store_file = client.vector_stores.files.delete(\n",
    "#     vector_store_id=vector_store_id,\n",
    "#     file_id=id_to_delete\n",
    "# )\n",
    "client.files.delete(id_to_delete)\n",
    "print(f\"Deleted file with ID: {id_to_delete}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b3fee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_file_in_vector_store(\n",
    "    client, vector_store_id, deleted_file_id, file_streams: list\n",
    "):\n",
    "    try:\n",
    "        client.files.delete(deleted_file_id)\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting file {deleted_file_id}: {e}\")\n",
    "    update_file = client.vector_stores.file_batches.upload_and_poll(\n",
    "        vector_store_id=vector_store_id, files=file_streams\n",
    "    )\n",
    "\n",
    "    return update_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdc2a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_file = client.vector_stores.file_batches.upload_and_poll(\n",
    "    vector_store_id=vector_store_id, files=file_streams[1:2]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e40fb0d",
   "metadata": {},
   "source": [
    "## Example retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeddb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.vector_stores.search(\n",
    "    vector_store_id=vector_store_id, query=\"youtube\", max_num_results=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a07a71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rich.print(response.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4426ed0f",
   "metadata": {},
   "source": [
    "# Check for new updates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348ddf0f",
   "metadata": {},
   "source": [
    "## Connect to Supabase PostgreSQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b2ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from supabase import create_client, Client\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "url: str = os.environ.get(\"SUPABASE_URL\")\n",
    "key: str = os.environ.get(\"SUPABASE_KEY\")\n",
    "supabase: Client = create_client(url, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54418904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "\n",
    "def get_hash_from_files(file_paths):\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    hashes = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "                sha256_hash.update(byte_block)\n",
    "        hashes.append(sha256_hash.hexdigest())\n",
    "\n",
    "    return hashes\n",
    "\n",
    "\n",
    "get_hash_from_files(\n",
    "    [\n",
    "        os.path.join(JSONL_DIR, file)\n",
    "        for file in os.listdir(JSONL_DIR)\n",
    "        if file.endswith(\".json\")\n",
    "    ][:4]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de42c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = supabase.table(\"scraped_articles\").select(\"hash\").execute()\n",
    "old_hashes = [item[\"hash\"] for item in response.data]\n",
    "old_hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948603cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "new_files = get_files_in_directory(JSONL_DIR)\n",
    "new_hashes = get_hash_from_files(new_files)\n",
    "\n",
    "delta_files = [\n",
    "    {\n",
    "        \"id\": file.split(os.sep)[-1],\n",
    "        \"hash\": hash_value,\n",
    "        \"updated_at\": datetime.now(timezone.utc).isoformat(),\n",
    "    }\n",
    "    for file, hash_value in zip(new_files, new_hashes)\n",
    "    if hash_value not in old_hashes\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57110dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_files[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e5c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "now_utc = datetime.now(timezone.utc).isoformat()\n",
    "print(now_utc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
